{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Okay now with ISEAR???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading ISEAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load ISEAR dataset\n",
    "data = pd.read_csv('/home/pes1ug22am100/Documents/Research and Experimentation/Learning-Machine-Unlearning/eng_dataset.csv')\n",
    "# Map emotions to binary labels (positive: 1, negative: 0)\n",
    "positive_emotions = [\"joy\"]\n",
    "negative_emotions = [\"fear\", \"anger\", \"sadness\", \"disgust\", \"shame\", \"guilt\"]\n",
    "\n",
    "df = data.copy()\n",
    "df[\"label\"] = df[\"sentiment\"].apply(lambda x: 1 if x in positive_emotions else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10941</td>\n",
       "      <td>anger</td>\n",
       "      <td>At the point today where if someone says somet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10942</td>\n",
       "      <td>anger</td>\n",
       "      <td>@CorningFootball  IT'S GAME DAY!!!!      T MIN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10943</td>\n",
       "      <td>anger</td>\n",
       "      <td>This game has pissed me off more than any othe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10944</td>\n",
       "      <td>anger</td>\n",
       "      <td>@spamvicious I've just found out it's Candice ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10945</td>\n",
       "      <td>anger</td>\n",
       "      <td>@moocowward @mrsajhargreaves @Melly77 @GaryBar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID sentiment                                            content\n",
       "0  10941     anger  At the point today where if someone says somet...\n",
       "1  10942     anger  @CorningFootball  IT'S GAME DAY!!!!      T MIN...\n",
       "2  10943     anger  This game has pissed me off more than any othe...\n",
       "3  10944     anger  @spamvicious I've just found out it's Candice ...\n",
       "4  10945     anger  @moocowward @mrsajhargreaves @Melly77 @GaryBar..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10941</td>\n",
       "      <td>anger</td>\n",
       "      <td>At the point today where if someone says somet...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10942</td>\n",
       "      <td>anger</td>\n",
       "      <td>@CorningFootball  IT'S GAME DAY!!!!      T MIN...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10943</td>\n",
       "      <td>anger</td>\n",
       "      <td>This game has pissed me off more than any othe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10944</td>\n",
       "      <td>anger</td>\n",
       "      <td>@spamvicious I've just found out it's Candice ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10945</td>\n",
       "      <td>anger</td>\n",
       "      <td>@moocowward @mrsajhargreaves @Melly77 @GaryBar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID sentiment                                            content  label\n",
       "0  10941     anger  At the point today where if someone says somet...      0\n",
       "1  10942     anger  @CorningFootball  IT'S GAME DAY!!!!      T MIN...      0\n",
       "2  10943     anger  This game has pissed me off more than any othe...      0\n",
       "3  10944     anger  @spamvicious I've just found out it's Candice ...      0\n",
       "4  10945     anger  @moocowward @mrsajhargreaves @Melly77 @GaryBar...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label distribution:\n",
      "label\n",
      "0    5486\n",
      "1    1616\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLabel distribution:\")\n",
    "print(df[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-02 14:48:18.107130: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-02 14:48:18.118069: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1738487898.129738   27815 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1738487898.134248   27815 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-02 14:48:18.145283: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example preprocessed sequences:\n",
      "[[  23    1  438   89  190   33  113  369  140 8045  490    2   15    4\n",
      "  8046   42  491   41    7   10  679    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0]\n",
      " [4214   34  211   59  567 1822 1284  650  479    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0]\n",
      " [  18  211   72 1182   15   99   67  116  146  212  211   18  275   10\n",
      "   801    6  465   61    2  370   11   99 8047    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenize sentences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df[\"content\"])\n",
    "sequences = tokenizer.texts_to_sequences(df[\"content\"])\n",
    "\n",
    "# Pad sequences to a fixed length\n",
    "max_sequence_length = 50\n",
    "X = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# Labels\n",
    "y = np.array(df[\"label\"])\n",
    "\n",
    "# Print example preprocessed data\n",
    "print(\"\\nExample preprocessed sequences:\")\n",
    "print(X[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RNN time hehehe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pes1ug22am100/.local/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "W0000 00:00:1738487900.021250   27815 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "\n",
    "# Define RNN model\n",
    "def build_rnn_model(vocab_size, embedding_dim, max_sequence_length):\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length),\n",
    "        SimpleRNN(64, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Build model\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Vocabulary size\n",
    "embedding_dim = 50\n",
    "model = build_rnn_model(vocab_size, embedding_dim, max_sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Apply the SISA from another MISA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Shard the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_shards = 5\n",
    "shard_size = len(X) // num_shards\n",
    "\n",
    "# Split dataset into shards\n",
    "shards_X = [X[i * shard_size:(i + 1) * shard_size] for i in range(num_shards)]\n",
    "shards_y = [y[i * shard_size:(i + 1) * shard_size] for i in range(num_shards)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Train models on each shard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model on shard 1\n",
      "Training model on shard 2\n",
      "Training model on shard 3\n",
      "Training model on shard 4\n",
      "Training model on shard 5\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "for i in range(num_shards):\n",
    "    print(f\"Training model on shard {i+1}\")\n",
    "    model = build_rnn_model(vocab_size, embedding_dim, max_sequence_length)\n",
    "    model.fit(shards_X[i], shards_y[i], epochs=10, batch_size=32, verbose=0)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Quick Eval before unlearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x79fa8df16710> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 78ms/stepWARNING:tensorflow:6 out of the last 12 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x79fa8df16710> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Accuracy before unlearning: 77.00%\n"
     ]
    }
   ],
   "source": [
    "# Create a test set\n",
    "test_indices = np.random.choice(len(X), size=100, replace=False)\n",
    "X_test = X[test_indices]\n",
    "y_test = y[test_indices]\n",
    "\n",
    "# Aggregate predictions from all shard models\n",
    "def aggregate_predictions(models, X_test):\n",
    "    predictions = np.zeros((X_test.shape[0], 1))\n",
    "    for model in models:\n",
    "        predictions += model.predict(X_test)\n",
    "    return (predictions / len(models)) > 0.5  # Binary classification threshold\n",
    "\n",
    "# Evaluate accuracy\n",
    "y_pred_before = aggregate_predictions(models, X_test)\n",
    "accuracy_before = np.mean(y_pred_before.flatten() == y_test)\n",
    "print(f\"Accuracy before unlearning: {accuracy_before * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Now we unlearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Identofy shard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_point_index = 42  # Example index of data point to unlearn\n",
    "shard_index = data_point_index // shard_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Retrain shard model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard size before removal: 1420\n",
      "Shard size after removal: 1419\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x79fa08d9df90>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the data point was removed\n",
    "print(f\"Shard size before removal: {len(shards_X[shard_index])}\")\n",
    "new_shard_X = np.delete(shards_X[shard_index], data_point_index % shard_size, axis=0)\n",
    "new_shard_y = np.delete(shards_y[shard_index], data_point_index % shard_size, axis=0)\n",
    "print(f\"Shard size after removal: {len(new_shard_X)}\")\n",
    "\n",
    "# Retrain the model on the updated shard\n",
    "models[shard_index] = build_rnn_model(vocab_size, embedding_dim, max_sequence_length)\n",
    "models[shard_index].fit(new_shard_X, new_shard_y, epochs=10, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "Prediction for unlearned data point: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Get the model's prediction for the unlearned data point\n",
    "prediction = models[shard_index].predict(np.array([unlearned_X]))\n",
    "print(f\"Prediction for unlearned data point: {prediction[0][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Prediction before unlearning: 0.0000\n",
      "Prediction after unlearning: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Train a model on the original shard (including the unlearned data point)\n",
    "original_model = build_rnn_model(vocab_size, embedding_dim, max_sequence_length)\n",
    "original_model.fit(shards_X[shard_index], shards_y[shard_index], epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "# Get the model's prediction before unlearning\n",
    "prediction_before = original_model.predict(np.array([unlearned_X]))\n",
    "\n",
    "# Get the model's prediction after unlearning\n",
    "prediction_after = models[shard_index].predict(np.array([unlearned_X]))\n",
    "\n",
    "print(f\"Prediction before unlearning: {prediction_before[0][0]:.4f}\")\n",
    "print(f\"Prediction after unlearning: {prediction_after[0][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Prediction with unlearned data point: 0.0000\n",
      "Prediction without unlearned data point: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Duplicate the unlearned data point in the shard\n",
    "shards_X[shard_index] = np.vstack([shards_X[shard_index], unlearned_X])\n",
    "shards_y[shard_index] = np.append(shards_y[shard_index], unlearned_y)\n",
    "\n",
    "# Retrain the model on the shard with the duplicated data point\n",
    "original_model.fit(shards_X[shard_index], shards_y[shard_index], epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "# Remove the duplicated data point and retrain\n",
    "new_shard_X = np.delete(shards_X[shard_index], -1, axis=0)  # Remove the last duplicated data point\n",
    "new_shard_y = np.delete(shards_y[shard_index], -1, axis=0)\n",
    "unlearned_model.fit(new_shard_X, new_shard_y, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "# Compare predictions for the unlearned data point\n",
    "prediction_original = original_model.predict(np.array([unlearned_X]))\n",
    "prediction_unlearned = unlearned_model.predict(np.array([unlearned_X]))\n",
    "\n",
    "print(f\"Prediction with unlearned data point: {prediction_original[0][0]:.4f}\")\n",
    "print(f\"Prediction without unlearned data point: {prediction_unlearned[0][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. How well did it unlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Accuracy after unlearning: 77.00%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate accuracy after unlearning\n",
    "y_pred_after = aggregate_predictions(models, X_test)\n",
    "accuracy_after = np.mean(y_pred_after.flatten() == y_test)\n",
    "print(f\"Accuracy after unlearning: {accuracy_after * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final conclushuns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy before unlearning: 77.00%\n",
      "Accuracy after unlearning: 77.00%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy before unlearning: {accuracy_before * 100:.2f}%\")\n",
    "print(f\"Accuracy after unlearning: {accuracy_after * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# But wait did it actually unlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unlearned Data Point:\n",
      "Text: @TheBarmyArmy looking forward to div 2 next year @AndyBarmyArmy?  #leictershireaway #therey\n",
      "Label: 0\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the unlearned data point\n",
    "unlearned_X = shards_X[shard_index][data_point_index % shard_size]\n",
    "unlearned_y = shards_y[shard_index][data_point_index % shard_size]\n",
    "\n",
    "# Print the unlearned data point\n",
    "print(\"Unlearned Data Point:\")\n",
    "print(f\"Text: {df.iloc[data_point_index]['content']}\")\n",
    "print(f\"Label: {unlearned_y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "Prediction for unlearned data point: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Get the model's prediction for the unlearned data point\n",
    "prediction = models[shard_index].predict(np.array([unlearned_X]))\n",
    "print(f\"Prediction for unlearned data point: {prediction[0][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Prediction before unlearning: 0.0000\n",
      "Prediction after unlearning: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Train a model on the original shard (including the unlearned data point)\n",
    "original_model = build_rnn_model(vocab_size, embedding_dim, max_sequence_length)\n",
    "original_model.fit(shards_X[shard_index], shards_y[shard_index], epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "# Get the model's prediction before unlearning\n",
    "prediction_before = original_model.predict(np.array([unlearned_X]))\n",
    "\n",
    "# Get the model's prediction after unlearning\n",
    "prediction_after = models[shard_index].predict(np.array([unlearned_X]))\n",
    "\n",
    "print(f\"Prediction before unlearning: {prediction_before[0][0]:.4f}\")\n",
    "print(f\"Prediction after unlearning: {prediction_after[0][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Accuracy before unlearning: 77.00%\n",
      "Accuracy after unlearning: 77.00%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate accuracy before unlearning\n",
    "y_pred_before = aggregate_predictions(models, X_test)\n",
    "accuracy_before = np.mean(y_pred_before.flatten() == y_test)\n",
    "\n",
    "# Evaluate accuracy after unlearning\n",
    "y_pred_after = aggregate_predictions(models, X_test)\n",
    "accuracy_after = np.mean(y_pred_after.flatten() == y_test)\n",
    "\n",
    "print(f\"Accuracy before unlearning: {accuracy_before * 100:.2f}%\")\n",
    "print(f\"Accuracy after unlearning: {accuracy_after * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "Prediction with unlearned data point: 0.0000\n",
      "Prediction without unlearned data point: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Retrain the model on the entire shard (including the unlearned data point)\n",
    "original_model.fit(shards_X[shard_index], shards_y[shard_index], epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "# Retrain the model on the shard excluding the unlearned data point\n",
    "new_shard_X = np.delete(shards_X[shard_index], data_point_index % shard_size, axis=0)\n",
    "new_shard_y = np.delete(shards_y[shard_index], data_point_index % shard_size, axis=0)\n",
    "unlearned_model = build_rnn_model(vocab_size, embedding_dim, max_sequence_length)\n",
    "unlearned_model.fit(new_shard_X, new_shard_y, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "# Compare predictions for the unlearned data point\n",
    "prediction_original = original_model.predict(np.array([unlearned_X]))\n",
    "prediction_unlearned = unlearned_model.predict(np.array([unlearned_X]))\n",
    "\n",
    "print(f\"Prediction with unlearned data point: {prediction_original[0][0]:.4f}\")\n",
    "print(f\"Prediction without unlearned data point: {prediction_unlearned[0][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More verifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard size before removal: 1420\n",
      "Shard size after removal: 1419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pes1ug22am100/.local/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x79fa0b5be4a0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
